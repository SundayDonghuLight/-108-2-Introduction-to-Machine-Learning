{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6> Introduction to Machine Learning Homework </font>\n",
    "\n",
    "<font size=6> 　　Lab5 - Recurrent Neural Network </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>本次作業主要分成兩個部份，第一部份要請同學們手刻一遍LSTM，加強理解其運作的核心想法；</p>\n",
    "<p>第二部份則帶往實際應用面，練習去跑開源在github上的code，並有效率的修改替換所使用data。相關內容可以回顧 (4/28) 和 (5/01) 的課程影片。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0000ff size=5> 1. LSTM Implementation (50%)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>在這邊我們的目標是透過numpy來實現LSTM(Long Short-Term Memory)網路的前向傳遞，請參考以下LSTM reference card</p>\n",
    "<p>來完成input gate, output gate, forget gate這三個控制單元，並填入挖空的jupyter-notebook樣板中執行測資確認。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image1](https://github.com/SundayDonghuLight/108-2_Introduction-to-Machine-Learning/blob/master/Lab5/figure/LSTM-ref-card.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image2](https://github.com/SundayDonghuLight/108-2_Introduction-to-Machine-Learning/blob/master/Lab5/figure/LSTM-demo.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>以下LSTM_demo class是不需要改動的部份，將被後續的LSTM class繼承。</p>\n",
    "<p>這邊已經先設定好了網路的weights，請配合reference card和下方名稱用在各自對應的地方，</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_demo():\n",
    "    # Input size: 9  Hidden size: 2  Num layers: 1\n",
    "    def __init__(self):\n",
    "        # Forget gate weights\n",
    "        self.W_hf = np.array([[0, 0], [0, 0]])\n",
    "        self.B_hf = np.array([50, 50])\n",
    "        self.W_xf = np.array([[0, 0, 0, 0, 0, 0, 0, -1000, -1000], [0, 0, 0, 0, 0, 0, 0, -1000, 0]])\n",
    "        self.B_xf = np.array([50, 50])\n",
    "        # Input gate weights\n",
    "        self.W_hi = np.array([[0, 0], [0, 0]])\n",
    "        self.B_hi = np.array([0, 0])\n",
    "        self.W_xi = np.array([[50, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 50]])\n",
    "        self.B_xi = np.array([0, 0])\n",
    "        self.W_hl = np.array([[0, 0], [0, 0]])\n",
    "        self.B_hl = np.array([0, 0])\n",
    "        self.W_xl = np.array([[50, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 50]])\n",
    "        self.B_xl = np.array([0, 0])\n",
    "        # Output gate weights\n",
    "        self.W_ho = np.array([[0, -500], [0, 0]])\n",
    "        self.B_ho = np.array([0, 0])\n",
    "        self.W_xo = np.array([[-1.33, 50, -0.78, -0.34, 0.5, 1.33, 0.01, 0, 0], [0, 0.65, -0.78, -0.34, 0.5, 0.08, 0, 0, -1.33]])\n",
    "        self.B_xo = np.array([0, 0])\n",
    "    \n",
    "    def forget_gate(self, x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state): pass\n",
    "    \n",
    "    def input_gate(self, x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl): pass\n",
    "    \n",
    "    def cell_state(self, forget_gate_output, input_gate_output): pass\n",
    "    \n",
    "    def output_gate(self, x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state): pass\n",
    "    \n",
    "    def classifier(self, h):\n",
    "        if np.around(h[0], 3) == 0.208:\n",
    "            return [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "        elif np.around(h[1], 3) == 0.208:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.76:\n",
    "            return [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.16:\n",
    "            return [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.20:\n",
    "            return [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.30:\n",
    "            return [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.40:\n",
    "            return [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.60:\n",
    "            return [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "        elif np.around(h[0]+h[1], 2) == 0.50:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        else:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "    def __call__(self, x, h_c):\n",
    "        h = h_c[0]\n",
    "        c = h_c[1]\n",
    "        \n",
    "        f = self.forget_gate(x, h, self.W_hf, self.B_hf, self.W_xf, self.B_xf, c)\n",
    "        i = self.input_gate(x, h, self.W_hi, self.B_hi, self.W_xi, self.B_xi, \\\n",
    "                             self.W_hl, self.B_hl, self.W_xl, self.B_xl,)\n",
    "        c = self.cell_state(f, i)\n",
    "        h = self.output_gate(x, h, self.W_ho, self.B_ho, self.W_xo, self.B_xo, c)\n",
    "        \n",
    "        h_c = (h, c)\n",
    "        output = self.classifier(h)\n",
    "        \n",
    "        return output, h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(LSTM_demo):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forget_gate(self, x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):\n",
    "        # need to be implemented\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def input_gate(self, x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):\n",
    "        # need to be implemented\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def cell_state(self, forget_gate_output, input_gate_output):\n",
    "        # need to be implemented\n",
    "        \n",
    "    \n",
    "    def output_gate(self, x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):\n",
    "        # need to be implemented\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>測資介紹與LSTM：</font>\n",
    "<p>\"老吾老以及人之老，幼吾幼以及人之幼。\"</p>\n",
    "<p>我們將使用建構出的LSTM來預測上面這段國學名句，Input為第一個字'老'，而model將會預測出下一個字應該是接什麼，再把下一個丟進去即可預測下下個，依此類推。</p>\n",
    "<p>在自然語言處理領域常會採用word embedding或稱word vector的技術，即把每一個字都對應到一個向量來表示，一種簡單的方式是使用one-hot encoding，總共有幾個不同的字就採用多少維的向量，每個字對應到只有一個元素為1其他為0的向量。</p>\n",
    "<p>這邊定義以下兩個轉換函數</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector(word):\n",
    "    if word == '老':\n",
    "        vector = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif word == '吾':\n",
    "        vector = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif word == '以':\n",
    "        vector = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    elif word == '及':\n",
    "        vector = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "    elif word == '人':\n",
    "        vector = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    elif word == '之':\n",
    "        vector = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "    elif word == '，':\n",
    "        vector = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "    elif word == '。':\n",
    "        vector = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "    elif word == '幼':\n",
    "        vector = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    else:\n",
    "        vector = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector2word(vector):\n",
    "    if vector == [1, 0, 0, 0, 0, 0, 0, 0, 0]:\n",
    "        word = '老'\n",
    "    elif vector == [0, 1, 0, 0, 0, 0, 0, 0, 0]:\n",
    "        word = '吾'\n",
    "    elif vector == [0, 0, 1, 0, 0, 0, 0, 0, 0]:\n",
    "        word = '以'\n",
    "    elif vector == [0, 0, 0, 1, 0, 0, 0, 0, 0]:\n",
    "        word = '及'\n",
    "    elif vector == [0, 0, 0, 0, 1, 0, 0, 0, 0]:\n",
    "        word = '人'\n",
    "    elif vector == [0, 0, 0, 0, 0, 1, 0, 0, 0]:\n",
    "        word = '之'\n",
    "    elif vector == [0, 0, 0, 0, 0, 0, 1, 0, 0]:\n",
    "        word = '，'\n",
    "    elif vector == [0, 0, 0, 0, 0, 0, 0, 1, 0]:\n",
    "        word = '。'\n",
    "    elif vector == [0, 0, 0, 0, 0, 0, 0, 0, 1]:\n",
    "        word = '幼'\n",
    "    else:\n",
    "        word = 'UNKNOWN WORD'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>初始化hidden state與cell state為0 並執行測資</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()\n",
    "h_c = (np.zeros(2), np.zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老\n",
      "老吾\n",
      "老吾老\n",
      "老吾老以\n",
      "老吾老以及\n",
      "老吾老以及人\n",
      "老吾老以及人之\n",
      "老吾老以及人之老\n",
      "老吾老以及人之老，\n",
      "老吾老以及人之老，幼\n",
      "老吾老以及人之老，幼吾\n",
      "老吾老以及人之老，幼吾幼\n",
      "老吾老以及人之老，幼吾幼以\n",
      "老吾老以及人之老，幼吾幼以及\n",
      "老吾老以及人之老，幼吾幼以及人\n",
      "老吾老以及人之老，幼吾幼以及人之\n",
      "老吾老以及人之老，幼吾幼以及人之幼\n",
      "老吾老以及人之老，幼吾幼以及人之幼。\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\n",
    "x = '老'\n",
    "string = string + x\n",
    "\n",
    "while(1):\n",
    "    print(string)\n",
    "    x = word2vector(x)\n",
    "    output, h_c = lstm(x, h_c)\n",
    "    x = vector2word(output)\n",
    "    if x == 'UNKNOWN WORD': break\n",
    "    string = string + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>分析：</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image3](https://github.com/SundayDonghuLight/108-2_Introduction-to-Machine-Learning/blob/master/Lab5/figure/transition-diagram.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>可以看出相同的input有可能產生不同的output(e.g.,老,幼,吾)，所以用一般的NN是沒辦法處理的，可以使用能記下過去資訊的RNN系列方法來解決。其中LSTM有著負責長期記憶的long-term-memory(cell state)和短期記憶的short-term-memory(hidden state)，像 以->及->人->之 只需要看前一個input就能推出下一個是什麼，主要靠著短期記憶的部份來處理，而'老'和'幼'的下一個字會接什麼可能就需要參考較長時間的記憶資訊。</p>\n",
    "<p>以下從剛剛的例子來想像下LSTM三個控制單元的功能與可能做到的一些事，通常對於傳遞下去的cell state改變的較慢，只是調整下加上一些數值，而hidden state在不同節點往往有很大的區別，這邊的cell state就是拿來存'老'和'幼'出現過了幾次(認為需要長期記憶的資訊)。所以在處理時序問題時，hidden state常常拿來用作抽取出來的特徵，後續再丟到fully connected之類的網路中進行分類或其他應用。</p>\n",
    "<p>Input gate: 控制選擇性記憶，若當前的input是需要長期記憶的就讓它可以加進cell state中記下來，若比較不重要就少記點。</p>\n",
    "<p>Output gate: 控制輸出到下一個時間節點的hidden state，根據input和前一hidden state決定要從長期記憶的cell state取哪幾項及取多少。</p>\n",
    "<p>Forget gate: 選擇性的忘記cell state內容，如下面第9到第10行，當不再需要記憶'老'出現幾次時就將其遺忘，方便更好的控制模型輸出。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   input      old_hidden        old_cell        output_hidden\n",
      "0  (老,[0. 0.]                 ,[0. 0.]) -> [0.15929455 0.        ]\n",
      "1  (吾,[0.15929455 0.        ] ,[1. 0.]) -> [0.76159416 0.        ]\n",
      "2  (老,[0.76159416 0.        ] ,[1. 0.]) -> [0.2016354 0.       ]\n",
      "3  (以,[0.2016354 0.       ]   ,[2. 0.]) -> [0.30301304 0.        ]\n",
      "4  (及,[0.30301304 0.        ] ,[2. 0.]) -> [0.4008518 0.       ]\n",
      "5  (人,[0.4008518 0.       ]   ,[2. 0.]) -> [0.60006796 0.        ]\n",
      "6  (之,[0.60006796 0.        ] ,[2. 0.]) -> [0.76239218 0.        ]\n",
      "7  (老,[0.76239218 0.        ] ,[2. 0.]) -> [0.20812502 0.        ]\n",
      "8  (，,[0.20812502 0.        ] ,[3. 0.]) -> [0.50001499 0.        ]\n",
      "9  (幼,[0.50001499 0.        ] ,[3. 0.]) -> [0.         0.15929455]\n",
      "10 (吾,[0.         0.15929455] ,[0. 1.]) -> [0.         0.50037533]\n",
      "11 (幼,[0.         0.50037533] ,[0. 1.]) -> [0.        0.2016354]\n",
      "12 (以,[0.        0.2016354]   ,[0. 2.]) -> [0.         0.30301304]\n",
      "13 (及,[0.         0.30301304] ,[0. 2.]) -> [0.        0.4008518]\n",
      "14 (人,[0.        0.4008518]   ,[0. 2.]) -> [0.         0.60006796]\n",
      "15 (之,[0.         0.60006796] ,[0. 2.]) -> [0.         0.50128407]\n",
      "16 (幼,[0.         0.50128407] ,[0. 2.]) -> [0.         0.20812502]\n",
      "17 (。,[0.         0.20812502] ,[0. 3.]) -> [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('   input      old_hidden        old_cell        output_hidden')\n",
    "\n",
    "for i, word in enumerate(string):\n",
    "    x = word2vector(word)\n",
    "    old_hc = h_c\n",
    "    output, h_c = lstm(x, h_c)\n",
    "    print('{:<2} ({},{:<24},{}) -> {}'.format(i, word, str(old_hc[0]), str(old_hc[1]), h_c[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>為了避免造成誤導再次聲明下，這邊只是舉個例子來表達LSTM是有可能做到類似這樣的行為的，實際上機器並不會用人類的方式思考，想像成先準備好了一個具備這種能力的模型，再透過最佳化loss function的方式調出最好最合適的參數。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0000ff size=5> 2. Rnn Application - Sequence to Sequence Chatbot (50%)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot source code: [https://github.com/ywk991112/pytorch-chatbot](https://github.com/ywk991112/pytorch-chatbot)\n",
    "Ptt Chinese corpus: [https://github.com/zake7749/Gossiping-Chinese-Corpus](https://github.com/zake7749/Gossiping-Chinese-Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>這題要做的事情是把上面連結的Chatbot source code跑起來，然後這個聊天機器人的專案原本是跑在一個英文語料的電影字幕dataset上，改成可以train在上面連結PTT中文語料的dataset上。這邊要求使用<code>Gossiping-QA-Dataset.txt</code>這份資料，並將檔案放在<code>data</code>資料夾下。</p>\n",
    "<p>由於training時間偏長，這邊並不要求大家把它train完，只要執行<code>python main.py -tr data/Gossiping-QA-Dataset.txt -la 1 -hi 512 -lr 0.0001 -it 50000 -b 64 -p 500 -s 1000</code>能跑起來就算完成。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>環境安裝注意事項：</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>基本上一般在github上的專案都會列出他所使用到的套件版本或是提供其他的環境需求，套件的部份大概常見下面這兩種方式：</p>\n",
    "<p>1. 直接列出所需要用到的套件與其版本，通常只會列最重要的那幾個，然後版本沒什麼相容性問題的套件不太會列出其版本(如下圖)。常常有可能並沒有把所有會用到的套件都列上去，這時可以直接執行程式等報錯 no module name ... 再一個個安裝。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image4](https://github.com/SundayDonghuLight/108-2_Introduction-to-Machine-Learning/blob/master/Lab5/figure/list-requirement.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>2. 會附上由pip導出的requirement.txt或由anaconda導出的.yaml檔(如下圖)，檔案裡會詳細列出所有該環境中的套件與其使用的版本，可直接執行指令安裝。pip導出的requirement.txt會包含到蠻多跟其他機器環境本身相關的東西，所以通常都沒辦法順利裝好會報錯。一種作法是等有出錯時再把文檔中該套件的那行註解掉重新執行，並反覆直到不再報錯為止；若是anaconda導出的.yaml檔通常會比較順利，若也出現問題則用同樣的方式處理就好好。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image5](https://github.com/SundayDonghuLight/108-2_Introduction-to-Machine-Learning/blob/master/Lab5/figure/yaml-requirement.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<p></p>\n",
    "<p></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
